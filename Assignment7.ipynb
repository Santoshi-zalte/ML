{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqUhT0NVKVpliXP3vjbd9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santoshi-zalte/LP2_-program/blob/main/Assignment7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YF1HuyxBljF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e14ef6-288c-42e6-f0fa-88c0e9b8d646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Tokenization is the first step of text analytics. The process of breaking down the text paragraph into smaller chunks such as words orsentences is called tokenization\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_text=sent_tokenize(text)\n",
        "tokenized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryfIeXO2GcfD",
        "outputId": "85c41a21-15ac-4215-c25b-22f4a45dad14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenization is the first step of text analytics.',\n",
              " 'The process of breaking down the text paragraph into smaller chunks such as words orsentences is called tokenization']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(text)\n",
        "tokenized_word\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOtMDf-mLEi4",
        "outputId": "51fccfaf-54a4-4228-afa3-7441944ce011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenization',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'step',\n",
              " 'of',\n",
              " 'text',\n",
              " 'analytics',\n",
              " '.',\n",
              " 'The',\n",
              " 'process',\n",
              " 'of',\n",
              " 'breaking',\n",
              " 'down',\n",
              " 'the',\n",
              " 'text',\n",
              " 'paragraph',\n",
              " 'into',\n",
              " 'smaller',\n",
              " 'chunks',\n",
              " 'such',\n",
              " 'as',\n",
              " 'words',\n",
              " 'orsentences',\n",
              " 'is',\n",
              " 'called',\n",
              " 'tokenization']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_text=sent_tokenize(text)\n",
        "tokenized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sflxvR0zKUU_",
        "outputId": "c90ce9f2-56ca-4f5e-d479-109d7550328f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenization is the first step of text analytics.',\n",
              " 'The process of breaking down the text paragraph into smaller chunks such as words orsentences is called tokenization']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print stop words of english\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4qAEm1HMYVb",
        "outputId": "ed981ed9-22ad-4ac5-ea33-41274eda2a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'was', \"doesn't\", 'very', 'their', 'or', 'hasn', 'weren', \"won't\", 'your', 'off', 'once', 'you', \"didn't\", 'm', 'have', 'all', 'through', 'me', 'to', 'each', 'no', 'isn', \"you'd\", 'these', 'he', 'yourself', 'himself', \"needn't\", 'of', \"you'll\", 'before', 'been', 'i', 'more', 'about', 'shan', 'wouldn', 'were', 'any', 'down', 'won', 'this', 'its', 'll', 'why', 'just', 'couldn', \"she's\", 'ourselves', 'after', 'd', 'myself', 'into', 'further', 'ma', 'those', 'below', 'does', 'themselves', 'doing', 'nor', \"don't\", 'can', \"hadn't\", 'under', 'shouldn', \"it's\", 'how', 'hers', 'should', 'y', 'only', 'now', 'haven', 'has', 'on', 'such', 'where', 'we', 'who', 'here', 'having', 'it', \"couldn't\", \"weren't\", 'when', 'by', 'too', 'above', 'are', 'my', 've', 'am', 'itself', 'but', \"you're\", 'out', 'do', 'there', \"wouldn't\", 'as', 'theirs', 'be', 'whom', 'until', 'yourselves', 'what', 'them', 'herself', 'with', \"mustn't\", 'his', \"should've\", 'mightn', 'and', 'not', 'don', 'is', 'other', 'in', 'which', 'because', 'again', 're', 'some', 'aren', \"shouldn't\", 'ain', \"aren't\", 'mustn', \"haven't\", 'for', 'the', \"you've\", 'didn', 'doesn', 'while', 'so', 'up', 'from', \"wasn't\", 'same', 'than', \"that'll\", 'him', 'she', 'her', 'they', 'between', 'if', 'own', 'over', 'needn', 'against', 'then', 'that', 'both', 'wasn', 'at', 'an', 'had', 't', 'hadn', 'yours', \"shan't\", 'being', 'during', 'our', 'most', 'a', 'will', 'ours', 'few', \"isn't\", \"mightn't\", 'did', 'o', 's', \"hasn't\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"How to remove stop words with NLTK library in Python?\"\n",
        "import re\n",
        "text=re.sub('[^a-zA-Z]',' ',text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GlADsjpfMoGb",
        "outputId": "b6f47e42-039f-4b59-bb48-b9419de0bf54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How to remove stop words with NLTK library in Python '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "stop_words=set (stopwords.words (\"english\")) \n",
        "print (stop_words)\n",
        "\n",
        "text= \"How to remove stop words with NLTK library in Python?\"\n",
        "text= re.sub ('[^a-zA-Z]',' ',text)\n",
        "tokens = word_tokenize (text.lower())\n",
        "filtered_text=[] \n",
        "for w in tokens:\n",
        "\n",
        "    if w not in stop_words: \n",
        "      filtered_text.append (w)\n",
        "\n",
        "print (\"Tokenized Sentence:\", tokens) \n",
        "print(\"Filterd Sentence:\", filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aoo1HryJK5fy",
        "outputId": "00c4d984-a5ff-40b4-c6bc-4e462fb3a807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'was', \"doesn't\", 'very', 'their', 'or', 'hasn', 'weren', \"won't\", 'your', 'off', 'once', 'you', \"didn't\", 'm', 'have', 'all', 'through', 'me', 'to', 'each', 'no', 'isn', \"you'd\", 'these', 'he', 'yourself', 'himself', \"needn't\", 'of', \"you'll\", 'before', 'been', 'i', 'more', 'about', 'shan', 'wouldn', 'were', 'any', 'down', 'won', 'this', 'its', 'll', 'why', 'just', 'couldn', \"she's\", 'ourselves', 'after', 'd', 'myself', 'into', 'further', 'ma', 'those', 'below', 'does', 'themselves', 'doing', 'nor', \"don't\", 'can', \"hadn't\", 'under', 'shouldn', \"it's\", 'how', 'hers', 'should', 'y', 'only', 'now', 'haven', 'has', 'on', 'such', 'where', 'we', 'who', 'here', 'having', 'it', \"couldn't\", \"weren't\", 'when', 'by', 'too', 'above', 'are', 'my', 've', 'am', 'itself', 'but', \"you're\", 'out', 'do', 'there', \"wouldn't\", 'as', 'theirs', 'be', 'whom', 'until', 'yourselves', 'what', 'them', 'herself', 'with', \"mustn't\", 'his', \"should've\", 'mightn', 'and', 'not', 'don', 'is', 'other', 'in', 'which', 'because', 'again', 're', 'some', 'aren', \"shouldn't\", 'ain', \"aren't\", 'mustn', \"haven't\", 'for', 'the', \"you've\", 'didn', 'doesn', 'while', 'so', 'up', 'from', \"wasn't\", 'same', 'than', \"that'll\", 'him', 'she', 'her', 'they', 'between', 'if', 'own', 'over', 'needn', 'against', 'then', 'that', 'both', 'wasn', 'at', 'an', 'had', 't', 'hadn', 'yours', \"shan't\", 'being', 'during', 'our', 'most', 'a', 'will', 'ours', 'few', \"isn't\", \"mightn't\", 'did', 'o', 's', \"hasn't\"}\n",
            "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
            "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "e_words =[\"wait\", \"waiting\", \"waited\", \"waits\"] \n",
        "ps=PorterStemmer () \n",
        "for w in  e_words:\n",
        "     rootWord=ps.stem (w)\n",
        "print (rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3-7j8KVOBJ4",
        "outputId": "eb244557-97da-4113-d96b-34a677fd58d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk. stem import WordNetLemmatizer \n",
        "wordnet_lemmatizer=WordNetLemmatizer()\n",
        "text =\"studies studying cries cry\"\n",
        "tokenization= nltk.word_tokenize (text)\n",
        "for w in tokenization:\n",
        "       print (\"Lemma for () is {}\".format(w,\n",
        "       wordnet_lemmatizer.lemmatize (w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc5hV_Z7QEv0",
        "outputId": "5b9a862c-3255-4593-97bb-df1390799bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for () is studies\n",
            "Lemma for () is studying\n",
            "Lemma for () is cries\n",
            "Lemma for () is cry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "data=\"The pink sweater fit her perfectly\"\n",
        "words=word_tokenize (data)\n",
        "\n",
        "for word in words:\n",
        "    print(nltk.pos_tag([word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPchHR7LRpYN",
        "outputId": "1b73bd2b-73f0-4c0d-e53e-1b663c1a3ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT')]\n",
            "[('pink', 'NN')]\n",
            "[('sweater', 'NN')]\n",
            "[('fit', 'NN')]\n",
            "[('her', 'PRP$')]\n",
            "[('perfectly', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "ZFfE7nPgS0nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA= 'Jupiter-is the largest Planet!'\n",
        "\n",
        "documentB='Mars is the fourth planet from the Sun'"
      ],
      "metadata": {
        "id": "oDPflHMdTLTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWordsA =documentA.split(' ')\n",
        "bagOfWordsB =documentB.split(' ')"
      ],
      "metadata": {
        "id": "eMcBNpt4UXRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueWords =set (bagOfWordsA).union (set (bagOfWordsB))"
      ],
      "metadata": {
        "id": "BEA3pITgU5sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numOfWordsA= dict.fromkeys (uniqueWords, 0)\n",
        "\n",
        "for word in bagOfWordsA: \n",
        "  numOfWordsA[word] += 1\n",
        "  numOfWordsB= dict.fromkeys (uniqueWords, 0)\n",
        "\n",
        "for word in bagOfWordsB: \n",
        "  numOfWordsB[word] += 1"
      ],
      "metadata": {
        "id": "EZ6ZCPQmVNJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def computeIDF (documents):\n",
        "  N =len (documents)\n",
        "  idfDict= dict.fromkeys (documents[0].keys (), 0)\n",
        "  for document in documents: \n",
        "    for word, val in document.items ():   \n",
        "      if val > 0:\n",
        "        idfDict[word] +=1\n",
        "\n",
        "  for word, val in idfDict.items():\n",
        "    idfDict [word]= math.log (N / float (val))\n",
        "  return idfDict\n",
        "\n",
        "\n",
        "idfs = computeIDF([numOfWordsA,numOfWordsB])\n",
        "idfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMnjPUGiA9C_",
        "outputId": "001b054f-a53b-4363-bb91-bc41ff74c7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Jupiter-is': 0.6931471805599453,\n",
              " 'Mars': 0.6931471805599453,\n",
              " 'Planet!': 0.6931471805599453,\n",
              " 'Sun': 0.6931471805599453,\n",
              " 'fourth': 0.6931471805599453,\n",
              " 'from': 0.6931471805599453,\n",
              " 'is': 0.6931471805599453,\n",
              " 'largest': 0.6931471805599453,\n",
              " 'planet': 0.6931471805599453,\n",
              " 'the': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def computeTF(wordDict, bagOfWords):\n",
        "  tfDict= {}\n",
        "  bagOfWordsCount =len(bagOfWords)\n",
        "  for word, count in wordDict.items():\n",
        "    tfDict [word] =count /float (bagOfWordsCount)\n",
        "  return tfDict\n",
        "\n",
        "\n",
        "tfA= computeTF (numOfWordsA, bagOfWordsA)\n",
        "tfB= computeTF (numOfWordsB, bagOfWordsB) \n"
      ],
      "metadata": {
        "id": "iAeYRiGFWRNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTFIDF (tfBagOfWords, idfs):\n",
        " tfidf = {}\n",
        " for word, val in tfBagOfWords.items(): \n",
        "    tfidf[word]=val*idfs[word]\n",
        " return tfidf\n",
        "tfidfA= computeTFIDF(tfA,idfs)\n",
        "tfidfB= computeTFIDF(tfB,idfs)\n",
        "\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "3d4UDli-JWnY",
        "outputId": "c410ff2d-8fd9-4b09-9421-57860f4872da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Jupiter-is    fourth   Planet!    planet       Sun   largest      Mars  \\\n",
              "0    0.173287  0.000000  0.173287  0.000000  0.000000  0.173287  0.000000   \n",
              "1    0.000000  0.086643  0.000000  0.086643  0.086643  0.000000  0.086643   \n",
              "\n",
              "       from        is  the  \n",
              "0  0.000000  0.000000  0.0  \n",
              "1  0.086643  0.086643  0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4220f27a-d8cf-4c23-93ee-4544ac492c05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Jupiter-is</th>\n",
              "      <th>fourth</th>\n",
              "      <th>Planet!</th>\n",
              "      <th>planet</th>\n",
              "      <th>Sun</th>\n",
              "      <th>largest</th>\n",
              "      <th>Mars</th>\n",
              "      <th>from</th>\n",
              "      <th>is</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.173287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086643</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086643</td>\n",
              "      <td>0.086643</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086643</td>\n",
              "      <td>0.086643</td>\n",
              "      <td>0.086643</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4220f27a-d8cf-4c23-93ee-4544ac492c05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4220f27a-d8cf-4c23-93ee-4544ac492c05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4220f27a-d8cf-4c23-93ee-4544ac492c05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}